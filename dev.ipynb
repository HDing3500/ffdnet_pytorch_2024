{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. test_ffdnet_ipol.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dingh\\Desktop\\Indigo\\FFDNet-Test.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from models import FFDNet\n",
    "from utils import batch_psnr, normalize, init_logger_ipol, \\\n",
    "\t\t\t\tvariable_to_cv2_image, remove_dataparallel_wrapper, is_rgb\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "input= r\"C:\\Users\\dingh\\Desktop\\Indigo\\FFDNet-Test.png\"\n",
    "noise_sigma=25\n",
    "add_noise=False\n",
    "dont_save_results=True\n",
    "no_gpu=False\n",
    "\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Testing FFDNet model ###\n",
      "> Parameters:\n",
      "\tadd_noise: True\n",
      "\tinput: C:\\Users\\dingh\\Desktop\\Indigo\\FFDNet-Test.png\n",
      "\tsuffix: \n",
      "\tnoise_sigma: 0.09803921568627451\n",
      "\tdont_save_results: False\n",
      "\tno_gpu: False\n",
      "\tcuda: True\n",
      "\n",
      "\n",
      "rgb: True\n",
      "im shape: (1366, 2048, 3)\n",
      "Loading model ...\n",
      "\n",
      "model_fn c:\\Users\\dingh\\Desktop\\Indigo\\ffdnet-pytorch\\models/net_rgb.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dingh\\AppData\\Local\\Temp\\ipykernel_4452\\1771833235.py:109: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_image 'noisy.png' and 'ffdnet.png'\n",
      "saved_image 'noisy_diff.png' and 'ffdnet_diff.png'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Denoise an image with the FFDNet denoising method\n",
    "\n",
    "Copyright (C) 2018, Matias Tassano <matias.tassano@parisdescartes.fr>\n",
    "\n",
    "This program is free software: you can use, modify and/or\n",
    "redistribute it under the terms of the GNU General Public\n",
    "License as published by the Free Software Foundation, either\n",
    "version 3 of the License, or (at your option) any later\n",
    "version. You should have received a copy of this license along\n",
    "this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from models import FFDNet\n",
    "from utils import batch_psnr, normalize, init_logger_ipol, \\\n",
    "\t\t\t\tvariable_to_cv2_image, remove_dataparallel_wrapper, is_rgb\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "Input= r\"C:\\Users\\dingh\\Desktop\\Indigo\\FFDNet-Test.png\" #test image\n",
    "noise_sigma=25\n",
    "add_noise=False\n",
    "dont_save_results=True\n",
    "no_gpu=False\n",
    "\n",
    "\n",
    "# Normalize noises ot [0, 1]\n",
    "noise_sigma /= 255\n",
    "\n",
    "# use CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "def test_ffdnet(**args):\n",
    "    r\"\"\"Denoises an input image with FFDNet\n",
    "    \"\"\"\n",
    "    #r\"\"\"C:\\Users\\dingh\\Desktop\\Indigo\\FFDNet-Test.png\n",
    "    #\"\"\" \n",
    "    # Init logger\n",
    "    logger = init_logger_ipol()\n",
    "\n",
    "    # Check if input exists and if it is RGB\n",
    "    try:\n",
    "        rgb_den = is_rgb(args[\"input\"])\n",
    "\n",
    "    except:\n",
    "        raise Exception('Could not open the input image')\n",
    "\n",
    "    # Open image as a CxHxW torch.Tensor\n",
    "    if rgb_den:\n",
    "        in_ch = 3\n",
    "        model_fn = 'models/net_rgb.pth'\n",
    "        imorig = cv2.imread(args['input'])\n",
    "        # from HxWxC to CxHxW, RGB image\n",
    "        imorig = (cv2.cvtColor(imorig, cv2.COLOR_BGR2RGB)).transpose(2, 0, 1)\n",
    "    else:\n",
    "        # from HxWxC to  CxHxW grayscale image (C=1)\n",
    "        in_ch = 1\n",
    "        model_fn = 'models/net_gray.pth'\n",
    "        imorig = cv2.imread(args['input'], cv2.IMREAD_GRAYSCALE)\n",
    "        imorig = np.expand_dims(imorig, 0)\n",
    "    imorig = np.expand_dims(imorig, 0)\n",
    "\n",
    "    # Handle odd sizes\n",
    "    expanded_h = False\n",
    "    expanded_w = False\n",
    "    sh_im = imorig.shape\n",
    "    if sh_im[2]%2 == 1:\n",
    "        expanded_h = True\n",
    "        imorig = np.concatenate((imorig, \\\n",
    "                imorig[:, :, -1, :][:, :, np.newaxis, :]), axis=2)\n",
    "\n",
    "    if sh_im[3]%2 == 1:\n",
    "        expanded_w = True\n",
    "        imorig = np.concatenate((imorig, \\\n",
    "                imorig[:, :, :, -1][:, :, :, np.newaxis]), axis=3)\n",
    "\n",
    "    imorig = normalize(imorig)\n",
    "    imorig = torch.Tensor(imorig)\n",
    "\n",
    "    # Absolute path to model file\n",
    "    if \"model_pth\" in list(args.keys()):\n",
    "        model_fn = os.path.join(args[\"model_pth\"], model_fn)\n",
    "    else:\n",
    "        model_fn = os.path.join(os.getcwd(), model_fn)\n",
    "   \n",
    "\n",
    "    # model_fn = os.path.join(os.path.abspath(os.path.dirname(__file__)), \\\n",
    "\t# \t\t\tmodel_fn)\n",
    "\n",
    "    #TO DO: FIX HARD CODED IMAGE \n",
    "\n",
    "    # Create model\n",
    "    print('Loading model ...\\n')\n",
    "    net = FFDNet(num_input_channels=in_ch)\n",
    "\n",
    "    # Load saved weights\n",
    "    if args['cuda']:\n",
    "        print(\"model_fn\", model_fn)\n",
    "        state_dict = torch.load(model_fn)\n",
    "        device_ids = [0]\n",
    "        model = nn.DataParallel(net, device_ids=device_ids).cuda()\n",
    "    else:\n",
    "        state_dict = torch.load(model_fn, map_location='cpu')\n",
    "        # CPU mode: remove the DataParallel wrapper\n",
    "        state_dict = remove_dataparallel_wrapper(state_dict)\n",
    "        model = net\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Sets the model in evaluation mode (e.g. it removes BN)\n",
    "    model.eval()\n",
    "\n",
    "    # Sets data type according to CPU or GPU modes\n",
    "    if args['cuda']:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # Add noise\n",
    "    if args['add_noise']:\n",
    "        noise = torch.FloatTensor(imorig.size()).\\\n",
    "                normal_(mean=0, std=args['noise_sigma'])\n",
    "        imnoisy = imorig + noise\n",
    "    else:\n",
    "        imnoisy = imorig.clone()\n",
    "\n",
    "        # Test mode\n",
    "    with torch.no_grad(): # PyTorch v0.4.0\n",
    "        imorig, imnoisy = Variable(imorig.type(dtype)), \\\n",
    "                        Variable(imnoisy.type(dtype))\n",
    "        nsigma = Variable(\n",
    "                torch.FloatTensor([args['noise_sigma']]).type(dtype))\n",
    "\n",
    "    # Measure runtime\n",
    "    start_t = time.time()\n",
    "\n",
    "    # Estimate noise and subtract it to the input image\n",
    "    im_noise_estim = model(imnoisy, nsigma)\n",
    "    outim = torch.clamp(imnoisy-im_noise_estim, 0., 1.)\n",
    "    stop_t = time.time()\n",
    "\n",
    "    if expanded_h:\n",
    "        imorig = imorig[:, :, :-1, :]\n",
    "        outim = outim[:, :, :-1, :]\n",
    "        imnoisy = imnoisy[:, :, :-1, :]\n",
    "\n",
    "    if expanded_w:\n",
    "        imorig = imorig[:, :, :, :-1]\n",
    "        outim = outim[:, :, :, :-1]\n",
    "        imnoisy = imnoisy[:, :, :, :-1]\n",
    "\n",
    "    # Compute PSNR and log it\n",
    "    if rgb_den:\n",
    "        logger.info(\"### RGB denoising ###\")\n",
    "    else:\n",
    "        logger.info(\"### Grayscale denoising ###\")\n",
    "    if args['add_noise']:\n",
    "        psnr = batch_psnr(outim, imorig, 1.)\n",
    "        psnr_noisy = batch_psnr(imnoisy, imorig, 1.)\n",
    "\n",
    "        logger.info(\"\\tPSNR noisy {0:0.2f}dB\".format(psnr_noisy))\n",
    "        logger.info(\"\\tPSNR denoised {0:0.2f}dB\".format(psnr))\n",
    "    else:\n",
    "        logger.info(\"\\tNo noise was added, cannot compute PSNR\")\n",
    "    logger.info(\"\\tRuntime {0:0.4f}s\".format(stop_t-start_t))\n",
    "\n",
    "    # Compute difference\n",
    "    diffout   = 2*(outim - imorig) + .5\n",
    "    diffnoise = 2*(imnoisy-imorig) + .5\n",
    "\n",
    "    # Save images\n",
    "    if not args['dont_save_results']:\n",
    "        noisyimg = variable_to_cv2_image(imnoisy)\n",
    "        outimg = variable_to_cv2_image(outim)\n",
    "        cv2.imwrite(\"noisy.png\", noisyimg)\n",
    "        cv2.imwrite(\"ffdnet.png\", outimg)\n",
    "        print(\"saved_image 'noisy.png' and 'ffdnet.png'\")\n",
    "        if args['add_noise']:\n",
    "             cv2.imwrite(\"noisy_diff.png\", variable_to_cv2_image(diffnoise))\n",
    "             cv2.imwrite(\"ffdnet_diff.png\", variable_to_cv2_image(diffout))\n",
    "             print(\"saved_image 'noisy_diff.png' and 'ffdnet_diff.png'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse arguments\n",
    "    parser = argparse.ArgumentParser(description=\"FFDNet_Test\")\n",
    "    parser.add_argument('--add_noise', type=str, default=\"True\")\n",
    "    parser.add_argument(\"--input\", type=str, default=\"\", \\\n",
    "                        help='path to input image')\n",
    "    parser.add_argument(\"--suffix\", type=str, default=\"\", \\\n",
    "                        help='suffix to add to output name')\n",
    "    parser.add_argument(\"--noise_sigma\", type=float, default=25, \\\n",
    "                        help='noise level used on test set')\n",
    "    parser.add_argument(\"--dont_save_results\", action='store_true', \\\n",
    "                        help=\"don't save output images\")\n",
    "    parser.add_argument(\"--no_gpu\", action='store_true', \\\n",
    "                        help=\"run model on CPU\")\n",
    "    #parser.add_argument(\"model_pth\", type=str, default=None)\n",
    "    argspar = parser.parse_args(\"\")\n",
    "    # Normalize noises ot [0, 1]\n",
    "    argspar.noise_sigma /= 255.\n",
    "    \n",
    "    #give the directory for model_pth\n",
    "    #argspar.model_pth = r\"C:\\Users\\dingh\\Desktop\\Indigo\\ffdnet-pytorch\"    \n",
    "    \n",
    "    #Define input\n",
    "    argspar.input = Input \n",
    "\n",
    "    # String to bool\n",
    "    argspar.add_noise = (argspar.add_noise.lower() == 'true')\n",
    "\n",
    "    # use CUDA?\n",
    "    argspar.cuda = not argspar.no_gpu and torch.cuda.is_available()\n",
    "\n",
    "    print(\"\\n### Testing FFDNet model ###\")\n",
    "    print(\"> Parameters:\")\n",
    "    for p, v in zip(argspar.__dict__.keys(), argspar.__dict__.values()):\n",
    "        print('\\t{}: {}'.format(p, v))\n",
    "    print('\\n')\n",
    "\n",
    "    test_ffdnet(**vars(argspar))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dingh\\Desktop\\Indigo\\FFDNet-Test.png\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset related functions\n",
    "\n",
    "Copyright (C) 2018, Matias Tassano <matias.tassano@parisdescartes.fr>\n",
    "\n",
    "This program is free software: you can use, modify and/or\n",
    "redistribute it under the terms of the GNU General Public\n",
    "License as published by the Free Software Foundation, either\n",
    "version 3 of the License, or (at your option) any later\n",
    "version. You should have received a copy of this license along\n",
    "this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as udata\n",
    "from utils import data_augmentation, normalize\n",
    "\n",
    "def img_to_patches(img, win, stride=1):\n",
    "\tr\"\"\"Converts an image to an array of patches.\n",
    "\n",
    "\tArgs:\n",
    "\t\timg: a numpy array containing a CxHxW RGB (C=3) or grayscale (C=1)\n",
    "\t\t\timage\n",
    "\t\twin: size of the output patches\n",
    "\t\tstride: int. stride\n",
    "\t\"\"\"\n",
    "\tk = 0\n",
    "\tendc = img.shape[0]\n",
    "\tendw = img.shape[1]\n",
    "\tendh = img.shape[2]\n",
    "\tpatch = img[:, 0:endw-win+0+1:stride, 0:endh-win+0+1:stride]\n",
    "\ttotal_pat_num = patch.shape[1] * patch.shape[2]\n",
    "\tres = np.zeros([endc, win*win, total_pat_num], np.float32)\n",
    "\tfor i in range(win):\n",
    "\t\tfor j in range(win):\n",
    "\t\t\tpatch = img[:, i:endw-win+i+1:stride, j:endh-win+j+1:stride]\n",
    "\t\t\tres[:, k, :] = np.array(patch[:]).reshape(endc, total_pat_num)\n",
    "\t\t\tk = k + 1\n",
    "\treturn res.reshape([endc, win, win, total_pat_num])\n",
    "\n",
    "def prepare_data(data_path, \\\n",
    "\t\t\t\t val_data_path, \\\n",
    "\t\t\t\t patch_size, \\\n",
    "\t\t\t\t stride, \\\n",
    "\t\t\t\t max_num_patches=None, \\\n",
    "\t\t\t\t aug_times=1, \\\n",
    "\t\t\t\t gray_mode=False):\n",
    "\tr\"\"\"Builds the training and validations datasets by scanning the\n",
    "\tcorresponding directories for images and extracting\tpatches from them.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdata_path: path containing the training image dataset\n",
    "\t\tval_data_path: path containing the validation image dataset\n",
    "\t\tpatch_size: size of the patches to extract from the images\n",
    "\t\tstride: size of stride to extract patches\n",
    "\t\tstride: size of stride to extract patches\n",
    "\t\tmax_num_patches: maximum number of patches to extract\n",
    "\t\taug_times: number of times to augment the available data minus one\n",
    "\t\tgray_mode: build the databases composed of grayscale patches\n",
    "\t\"\"\"\n",
    "\t# training database\n",
    "\tprint('> Training database')\n",
    "\tscales = [1, 0.9, 0.8, 0.7]\n",
    "\ttypes = ('*.bmp', '*.png')\n",
    "\tfiles = []\n",
    "\tfor tp in types:\n",
    "\t\tfiles.extend(glob.glob(os.path.join(data_path, tp)))\n",
    "\tfiles.sort()\n",
    "\n",
    "\tif gray_mode:\n",
    "\t\ttraindbf = 'train_gray.h5'\n",
    "\t\tvaldbf = 'val_gray.h5'\n",
    "\telse:\n",
    "\t\ttraindbf = 'train_rgb.h5'\n",
    "\t\tvaldbf = 'val_rgb.h5'\n",
    "\n",
    "\tif max_num_patches is None:\n",
    "\t\tmax_num_patches = 5000000\n",
    "\t\tprint(\"\\tMaximum number of patches not set\")\n",
    "\telse:\n",
    "\t\tprint(\"\\tMaximum number of patches set to {}\".format(max_num_patches))\n",
    "\ttrain_num = 0\n",
    "\ti = 0\n",
    "\twith h5py.File(traindbf, 'w') as h5f:\n",
    "\t\twhile i < len(files) and train_num < max_num_patches:\n",
    "\t\t\timgor = cv2.imread(files[i])\n",
    "\t\t\t# h, w, c = img.shape\n",
    "\t\t\tfor sca in scales:\n",
    "\t\t\t\timg = cv2.resize(imgor, (0, 0), fx=sca, fy=sca, \\\n",
    "\t\t\t\t\t\t\t\tinterpolation=cv2.INTER_CUBIC)\n",
    "\t\t\t\tif not gray_mode:\n",
    "\t\t\t\t\t# CxHxW RGB image\n",
    "\t\t\t\t\timg = (cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).transpose(2, 0, 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# CxHxW grayscale image (C=1)\n",
    "\t\t\t\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\t\t\timg = np.expand_dims(img, 0)\n",
    "\t\t\t\timg = normalize(img)\n",
    "\t\t\t\tpatches = img_to_patches(img, win=patch_size, stride=stride)\n",
    "\t\t\t\tprint(\"\\tfile: %s scale %.1f # samples: %d\" % \\\n",
    "\t\t\t\t\t  (files[i], sca, patches.shape[3]*aug_times))\n",
    "\t\t\t\tfor nx in range(patches.shape[3]):\n",
    "\t\t\t\t\tdata = data_augmentation(patches[:, :, :, nx].copy(), \\\n",
    "\t\t\t\t\t\t\t  np.random.randint(0, 7))\n",
    "\t\t\t\t\th5f.create_dataset(str(train_num), data=data)\n",
    "\t\t\t\t\ttrain_num += 1\n",
    "\t\t\t\t\tfor mx in range(aug_times-1):\n",
    "\t\t\t\t\t\tdata_aug = data_augmentation(data, np.random.randint(1, 4))\n",
    "\t\t\t\t\t\th5f.create_dataset(str(train_num)+\"_aug_%d\" % (mx+1), data=data_aug)\n",
    "\t\t\t\t\t\ttrain_num += 1\n",
    "\t\t\ti += 1\n",
    "\n",
    "\t# validation database\n",
    "\tprint('\\n> Validation database')\n",
    "\tfiles = []\n",
    "\tfor tp in types:\n",
    "\t\tfiles.extend(glob.glob(os.path.join(val_data_path, tp)))\n",
    "\tfiles.sort()\n",
    "\th5f = h5py.File(valdbf, 'w')\n",
    "\tval_num = 0\n",
    "\tfor i, item in enumerate(files):\n",
    "\t\tprint(\"\\tfile: %s\" % item)\n",
    "\t\timg = cv2.imread(item)\n",
    "\t\tif not gray_mode:\n",
    "\t\t\t# C. H. W, RGB image\n",
    "\t\t\timg = (cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).transpose(2, 0, 1)\n",
    "\t\telse:\n",
    "\t\t\t# C, H, W grayscale image (C=1)\n",
    "\t\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\timg = np.expand_dims(img, 0)\n",
    "\t\timg = normalize(img)\n",
    "\t\th5f.create_dataset(str(val_num), data=img)\n",
    "\t\tval_num += 1\n",
    "\th5f.close()\n",
    "\n",
    "\tprint('\\n> Total')\n",
    "\tprint('\\ttraining set, # samples %d' % train_num)\n",
    "\tprint('\\tvalidation set, # samples %d\\n' % val_num)\n",
    "\n",
    "class Dataset(udata.Dataset):\n",
    "\tr\"\"\"Implements torch.utils.data.Dataset\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, train=True, gray_mode=False, shuffle=False):\n",
    "\t\tsuper(Dataset, self).__init__()\n",
    "\t\tself.train = train\n",
    "\t\tself.gray_mode = gray_mode\n",
    "\t\tif not self.gray_mode:\n",
    "\t\t\tself.traindbf = 'train_rgb.h5'\n",
    "\t\t\tself.valdbf = 'val_rgb.h5'\n",
    "\t\telse:\n",
    "\t\t\tself.traindbf = 'train_gray.h5'\n",
    "\t\t\tself.valdbf = 'val_gray.h5'\n",
    "\n",
    "\t\tif self.train:\n",
    "\t\t\th5f = h5py.File(self.traindbf, 'r')\n",
    "\t\telse:\n",
    "\t\t\th5f = h5py.File(self.valdbf, 'r')\n",
    "\t\tself.keys = list(h5f.keys())\n",
    "\t\tif shuffle:\n",
    "\t\t\trandom.shuffle(self.keys)\n",
    "\t\th5f.close()\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.keys)\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tif self.train:\n",
    "\t\t\th5f = h5py.File(self.traindbf, 'r')\n",
    "\t\telse:\n",
    "\t\t\th5f = h5py.File(self.valdbf, 'r')\n",
    "\t\tkey = self.keys[index]\n",
    "\t\tdata = np.array(h5f[key])\n",
    "\t\th5f.close()\n",
    "\t\treturn torch.Tensor(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions implementing custom NN layers\n",
    "\n",
    "Copyright (C) 2018, Matias Tassano <matias.tassano@parisdescartes.fr>\n",
    "\n",
    "This program is free software: you can use, modify and/or\n",
    "redistribute it under the terms of the GNU General Public\n",
    "License as published by the Free Software Foundation, either\n",
    "version 3 of the License, or (at your option) any later\n",
    "version. You should have received a copy of this license along\n",
    "this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.autograd import Function, Variable\n",
    "\n",
    "def concatenate_input_noise_map(input, noise_sigma):\n",
    "\tr\"\"\"Implements the first layer of FFDNet. This function returns a\n",
    "\ttorch.autograd.Variable composed of the concatenation of the downsampled\n",
    "\tinput image and the noise map. Each image of the batch of size CxHxW gets\n",
    "\tconverted to an array of size 4*CxH/2xW/2. Each of the pixels of the\n",
    "\tnon-overlapped 2x2 patches of the input image are placed in the new array\n",
    "\talong the first dimension.\n",
    "\n",
    "\tArgs:\n",
    "\t\tinput: batch containing CxHxW images\n",
    "\t\tnoise_sigma: the value of the pixels of the CxH/2xW/2 noise map\n",
    "\t\"\"\"\n",
    "\t# noise_sigma is a list of length batch_size\n",
    "\tN, C, H, W = input.size()\n",
    "\tdtype = input.type()\n",
    "\tsca = 2\n",
    "\tsca2 = sca*sca\n",
    "\tCout = sca2*C\n",
    "\tHout = H//sca\n",
    "\tWout = W//sca\n",
    "\tidxL = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "\n",
    "\t# Fill the downsampled image with zeros\n",
    "\tif 'cuda' in dtype:\n",
    "\t\tdownsampledfeatures = torch.cuda.FloatTensor(N, Cout, Hout, Wout).fill_(0)\n",
    "\telse:\n",
    "\t\tdownsampledfeatures = torch.FloatTensor(N, Cout, Hout, Wout).fill_(0)\n",
    "\n",
    "\t# Build the CxH/2xW/2 noise map\n",
    "\tnoise_map = noise_sigma.view(N, 1, 1, 1).repeat(1, C, Hout, Wout)\n",
    "\n",
    "\t# Populate output\n",
    "\tfor idx in range(sca2):\n",
    "\t\tdownsampledfeatures[:, idx:Cout:sca2, :, :] = \\\n",
    "\t\t\tinput[:, :, idxL[idx][0]::sca, idxL[idx][1]::sca]\n",
    "\n",
    "\t# concatenate de-interleaved mosaic with noise map\n",
    "\treturn torch.cat((noise_map, downsampledfeatures), 1)\n",
    "\n",
    "class UpSampleFeaturesFunction(Function):\n",
    "\tr\"\"\"Extends PyTorch's modules by implementing a torch.autograd.Function.\n",
    "\tThis class implements the forward and backward methods of the last layer\n",
    "\tof FFDNet. It basically performs the inverse of\n",
    "\tconcatenate_input_noise_map(): it converts each of the images of a\n",
    "\tbatch of size CxH/2xW/2 to images of size C/4xHxW\n",
    "\t\"\"\"\n",
    "\t@staticmethod\n",
    "\tdef forward(ctx, input):\n",
    "\t\tN, Cin, Hin, Win = input.size()\n",
    "\t\tdtype = input.type()\n",
    "\t\tsca = 2\n",
    "\t\tsca2 = sca*sca\n",
    "\t\tCout = Cin//sca2\n",
    "\t\tHout = Hin*sca\n",
    "\t\tWout = Win*sca\n",
    "\t\tidxL = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "\n",
    "\t\tassert (Cin%sca2 == 0), \\\n",
    "\t\t\t'Invalid input dimensions: number of channels should be divisible by 4'\n",
    "\n",
    "\t\tresult = torch.zeros((N, Cout, Hout, Wout)).type(dtype)\n",
    "\t\tfor idx in range(sca2):\n",
    "\t\t\tresult[:, :, idxL[idx][0]::sca, idxL[idx][1]::sca] = \\\n",
    "\t\t\t\tinput[:, idx:Cin:sca2, :, :]\n",
    "\n",
    "\t\treturn result\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef backward(ctx, grad_output):\n",
    "\t\tN, Cg_out, Hg_out, Wg_out = grad_output.size()\n",
    "\t\tdtype = grad_output.data.type()\n",
    "\t\tsca = 2\n",
    "\t\tsca2 = sca*sca\n",
    "\t\tCg_in = sca2*Cg_out\n",
    "\t\tHg_in = Hg_out//sca\n",
    "\t\tWg_in = Wg_out//sca\n",
    "\t\tidxL = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "\n",
    "\t\t# Build output\n",
    "\t\tgrad_input = torch.zeros((N, Cg_in, Hg_in, Wg_in)).type(dtype)\n",
    "\t\t# Populate output\n",
    "\t\tfor idx in range(sca2):\n",
    "\t\t\tgrad_input[:, idx:Cg_in:sca2, :, :] = \\\n",
    "\t\t\t\tgrad_output.data[:, :, idxL[idx][0]::sca, idxL[idx][1]::sca]\n",
    "\n",
    "\t\treturn Variable(grad_input)\n",
    "\n",
    "# Alias functions\n",
    "upsamplefeatures = UpSampleFeaturesFunction.apply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of the FFDNet model and its custom layers\n",
    "\n",
    "Copyright (C) 2018, Matias Tassano <matias.tassano@parisdescartes.fr>\n",
    "\n",
    "This program is free software: you can use, modify and/or\n",
    "redistribute it under the terms of the GNU General Public\n",
    "License as published by the Free Software Foundation, either\n",
    "version 3 of the License, or (at your option) any later\n",
    "version. You should have received a copy of this license along\n",
    "this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import functions\n",
    "\n",
    "class UpSampleFeatures(nn.Module):\n",
    "\tr\"\"\"Implements the last layer of FFDNet\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(UpSampleFeatures, self).__init__()\n",
    "\tdef forward(self, x):\n",
    "\t\treturn functions.upsamplefeatures(x)\n",
    "\n",
    "class IntermediateDnCNN(nn.Module):\n",
    "\tr\"\"\"Implements the middel part of the FFDNet architecture, which\n",
    "\tis basically a DnCNN net\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_features, middle_features, num_conv_layers):\n",
    "\t\tsuper(IntermediateDnCNN, self).__init__()\n",
    "\t\tself.kernel_size = 3\n",
    "\t\tself.padding = 1\n",
    "\t\tself.input_features = input_features\n",
    "\t\tself.num_conv_layers = num_conv_layers\n",
    "\t\tself.middle_features = middle_features\n",
    "\t\tif self.input_features == 5:\n",
    "\t\t\tself.output_features = 4 #Grayscale image\n",
    "\t\telif self.input_features == 15:\n",
    "\t\t\tself.output_features = 12 #RGB image\n",
    "\t\telse:\n",
    "\t\t\traise Exception('Invalid number of input features')\n",
    "\n",
    "\t\tlayers = []\n",
    "\t\tlayers.append(nn.Conv2d(in_channels=self.input_features,\\\n",
    "\t\t\t\t\t\t\t\tout_channels=self.middle_features,\\\n",
    "\t\t\t\t\t\t\t\tkernel_size=self.kernel_size,\\\n",
    "\t\t\t\t\t\t\t\tpadding=self.padding,\\\n",
    "\t\t\t\t\t\t\t\tbias=False))\n",
    "\t\tlayers.append(nn.ReLU(inplace=True))\n",
    "\t\tfor _ in range(self.num_conv_layers-2):\n",
    "\t\t\tlayers.append(nn.Conv2d(in_channels=self.middle_features,\\\n",
    "\t\t\t\t\t\t\t\t\tout_channels=self.middle_features,\\\n",
    "\t\t\t\t\t\t\t\t\tkernel_size=self.kernel_size,\\\n",
    "\t\t\t\t\t\t\t\t\tpadding=self.padding,\\\n",
    "\t\t\t\t\t\t\t\t\tbias=False))\n",
    "\t\t\tlayers.append(nn.BatchNorm2d(self.middle_features))\n",
    "\t\t\tlayers.append(nn.ReLU(inplace=True))\n",
    "\t\tlayers.append(nn.Conv2d(in_channels=self.middle_features,\\\n",
    "\t\t\t\t\t\t\t\tout_channels=self.output_features,\\\n",
    "\t\t\t\t\t\t\t\tkernel_size=self.kernel_size,\\\n",
    "\t\t\t\t\t\t\t\tpadding=self.padding,\\\n",
    "\t\t\t\t\t\t\t\tbias=False))\n",
    "\t\tself.itermediate_dncnn = nn.Sequential(*layers)\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.itermediate_dncnn(x)\n",
    "\t\treturn out\n",
    "\n",
    "class FFDNet(nn.Module):\n",
    "\tr\"\"\"Implements the FFDNet architecture\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, num_input_channels):\n",
    "\t\tsuper(FFDNet, self).__init__()\n",
    "\t\tself.num_input_channels = num_input_channels\n",
    "\t\tif self.num_input_channels == 1:\n",
    "\t\t\t# Grayscale image\n",
    "\t\t\tself.num_feature_maps = 64\n",
    "\t\t\tself.num_conv_layers = 15\n",
    "\t\t\tself.downsampled_channels = 5\n",
    "\t\t\tself.output_features = 4\n",
    "\t\telif self.num_input_channels == 3:\n",
    "\t\t\t# RGB image\n",
    "\t\t\tself.num_feature_maps = 96\n",
    "\t\t\tself.num_conv_layers = 12\n",
    "\t\t\tself.downsampled_channels = 15\n",
    "\t\t\tself.output_features = 12\n",
    "\t\telse:\n",
    "\t\t\traise Exception('Invalid number of input features')\n",
    "\n",
    "\t\tself.intermediate_dncnn = IntermediateDnCNN(\\\n",
    "\t\t\t\tinput_features=self.downsampled_channels,\\\n",
    "\t\t\t\tmiddle_features=self.num_feature_maps,\\\n",
    "\t\t\t\tnum_conv_layers=self.num_conv_layers)\n",
    "\t\tself.upsamplefeatures = UpSampleFeatures()\n",
    "\n",
    "\tdef forward(self, x, noise_sigma):\n",
    "\t\tconcat_noise_x = functions.concatenate_input_noise_map(\\\n",
    "\t\t\t\tx.data, noise_sigma.data)\n",
    "\t\tconcat_noise_x = Variable(concat_noise_x)\n",
    "\t\th_dncnn = self.intermediate_dncnn(concat_noise_x)\n",
    "\t\tpred_noise = self.upsamplefeatures(h_dncnn)\n",
    "\t\treturn pred_noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V. prepare_patches.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Building databases ###\n",
      "> Parameters:\n",
      "\tgray: False\n",
      "\tpatch_size: 44\n",
      "\tstride: 20\n",
      "\tmax_number_patches: 100\n",
      "\taug_times: 1\n",
      "\ttrainset_dir: C:\\Users\\dingh\\Desktop\\Indigo\\CBSD68-dataset-master\\CBSD68\\original_png\n",
      "\tvalset_dir: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\n",
      "\n",
      "\n",
      "> Training database\n",
      "\tMaximum number of patches set to 100\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\CBSD68-dataset-master\\CBSD68\\original_png\\0000.png scale 1.0 # samples: 308\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\CBSD68-dataset-master\\CBSD68\\original_png\\0000.png scale 0.9 # samples: 260\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\CBSD68-dataset-master\\CBSD68\\original_png\\0000.png scale 0.8 # samples: 198\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\CBSD68-dataset-master\\CBSD68\\original_png\\0000.png scale 0.7 # samples: 150\n",
      "\n",
      "> Validation database\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\1.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\10.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\11.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\12.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\13.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\14.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\15.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\16.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\17.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\18.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\19.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\2.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\20.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\21.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\22.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\23.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\24.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\3.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\4.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\5.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\6.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\7.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\8.png\n",
      "\tfile: C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\\9.png\n",
      "\n",
      "> Total\n",
      "\ttraining set, # samples 916\n",
      "\tvalidation set, # samples 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Construction of the training and validation databases\n",
    "\n",
    "Copyright (C) 2018, Matias Tassano <matias.tassano@parisdescartes.fr>\n",
    "\n",
    "This program is free software: you can use, modify and/or\n",
    "redistribute it under the terms of the GNU General Public\n",
    "License as published by the Free Software Foundation, either\n",
    "version 3 of the License, or (at your option) any later\n",
    "version. You should have received a copy of this license along\n",
    "this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import argparse\n",
    "from dataset import prepare_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tparser = argparse.ArgumentParser(description=\\\n",
    "\t\t\t\t\t\t\t\t  \"Building the training patch database\")\n",
    "\tparser.add_argument(\"--gray\", action='store_true',\\\n",
    "\t\t\t\t\t\thelp='prepare grayscale database instead of RGB')\n",
    "\t# Preprocessing parameters\n",
    "\tparser.add_argument(\"--patch_size\", \"--p\", type=int, default=44, \\\n",
    "\t\t\t\t\t help=\"Patch size\")\n",
    "\tparser.add_argument(\"--stride\", \"--s\", type=int, default=20, \\\n",
    "\t\t\t\t\t help=\"Size of stride\")\n",
    "\tparser.add_argument(\"--max_number_patches\", \"--m\", type=int, default=100, \\\n",
    "\t\t\t\t\t\thelp=\"Maximum number of patches\")\n",
    "\tparser.add_argument(\"--aug_times\", \"--a\", type=int, default=1, \\\n",
    "\t\t\t\t\t\thelp=\"How many times to perform data augmentation\")\n",
    "\t# Dirs\n",
    "\tparser.add_argument(\"--trainset_dir\", type=str, default=r\"C:\\Users\\dingh\\Desktop\\Indigo\\CBSD68-dataset-master\\CBSD68\\original_png\", \\\n",
    "\t\t\t\t\t help='path of trainset')\n",
    "\tparser.add_argument(\"--valset_dir\", type=str, default=r\"C:\\Users\\dingh\\Desktop\\Indigo\\Kodak-Lossless-True-Color-Image-Suite-master\\PhotoCD_PCD0992\", \\\n",
    "\t\t\t\t\t\t help='path of validation set')\n",
    "\targs = parser.parse_args(\"\")\n",
    "\n",
    "\tif args.gray:\n",
    "\t\tif args.trainset_dir is None:\n",
    "\t\t\targs.trainset_dir = 'data/gray/train'\n",
    "\t\tif args.valset_dir is None:\n",
    "\t\t\targs.valset_dir = 'data/gray/Set12'\n",
    "\telse:\n",
    "\t\tif args.trainset_dir is None:\n",
    "\t\t\targs.trainset_dir = 'data/rgb/CImageNet_expl'\n",
    "\t\tif args.valset_dir is None:\n",
    "\t\t\targs.valset_dir = 'data/rgb/Kodak24'\n",
    "\n",
    "\tprint(\"\\n### Building databases ###\")\n",
    "\tprint(\"> Parameters:\")\n",
    "\tfor p, v in zip(args.__dict__.keys(), args.__dict__.values()):\n",
    "\t\tprint('\\t{}: {}'.format(p, v))\n",
    "\tprint('\\n')\n",
    "\n",
    "\tprepare_data(args.trainset_dir,\\\n",
    "\t\t\t\t\targs.valset_dir,\\\n",
    "\t\t\t\t\targs.patch_size,\\\n",
    "\t\t\t\t\targs.stride,\\\n",
    "\t\t\t\t\targs.max_number_patches,\\\n",
    "\t\t\t\t\taug_times=args.aug_times,\\\n",
    "\t\t\t\t\tgray_mode=args.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VI. train.py\n",
    "changed line 170 loss[0] to loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Training FFDNet model ###\n",
      "> Parameters:\n",
      "\tgray: False\n",
      "\tlog_dir: logs\n",
      "\tbatch_size: 128\n",
      "\tepochs: 80\n",
      "\tresume_training: False\n",
      "\tmilestone: [50, 60]\n",
      "\tlr: 0.001\n",
      "\tno_orthog: False\n",
      "\tsave_every: 10\n",
      "\tsave_every_epochs: 5\n",
      "\tnoiseIntL: [0.0, 0.29411764705882354]\n",
      "\tval_noiseL: 0.09803921568627451\n",
      "\n",
      "\n",
      "> Loading dataset ...\n",
      "\t# of training samples: 916\n",
      "\n",
      "learning rate 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dingh\\Desktop\\Indigo\\ffdnet-pytorch\\utils.py:33: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  nn.init.kaiming_normal(lyr.weight.data, a=0, mode='fan_in')\n",
      "c:\\Users\\dingh\\Desktop\\Indigo\\ffdnet-pytorch\\utils.py:39: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  nn.init.constant(lyr.bias.data, 0.0)\n",
      "c:\\Users\\dingh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1][1/8] loss: 77.7016 PSNR_train: 20.6819\n",
      "\n",
      "[epoch 1] PSNR_val: 20.4358\n",
      "learning rate 0.001000\n",
      "[epoch 2][3/8] loss: 80.7904 PSNR_train: 19.6632\n",
      "\n",
      "[epoch 2] PSNR_val: 20.4295\n",
      "learning rate 0.001000\n",
      "[epoch 3][5/8] loss: 68.9312 PSNR_train: 18.8181\n",
      "\n",
      "[epoch 3] PSNR_val: 20.4296\n",
      "learning rate 0.001000\n",
      "[epoch 4][7/8] loss: 53.5521 PSNR_train: 18.8754\n",
      "\n",
      "[epoch 4] PSNR_val: 20.4320\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 5] PSNR_val: 20.4229\n",
      "learning rate 0.001000\n",
      "[epoch 6][1/8] loss: 29.1655 PSNR_train: 19.9964\n",
      "\n",
      "[epoch 6] PSNR_val: 20.4054\n",
      "learning rate 0.001000\n",
      "[epoch 7][3/8] loss: 22.8981 PSNR_train: 19.6613\n",
      "\n",
      "[epoch 7] PSNR_val: 20.4149\n",
      "learning rate 0.001000\n",
      "[epoch 8][5/8] loss: 20.5306 PSNR_train: 19.0550\n",
      "\n",
      "[epoch 8] PSNR_val: 20.3945\n",
      "learning rate 0.001000\n",
      "[epoch 9][7/8] loss: 14.6931 PSNR_train: 18.9147\n",
      "\n",
      "[epoch 9] PSNR_val: 20.6287\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 10] PSNR_val: 21.4120\n",
      "learning rate 0.001000\n",
      "[epoch 11][1/8] loss: 13.8024 PSNR_train: 19.7802\n",
      "\n",
      "[epoch 11] PSNR_val: 22.5402\n",
      "learning rate 0.001000\n",
      "[epoch 12][3/8] loss: 12.3952 PSNR_train: 22.7842\n",
      "\n",
      "[epoch 12] PSNR_val: 23.2681\n",
      "learning rate 0.001000\n",
      "[epoch 13][5/8] loss: 11.2809 PSNR_train: 23.1143\n",
      "\n",
      "[epoch 13] PSNR_val: 21.8301\n",
      "learning rate 0.001000\n",
      "[epoch 14][7/8] loss: 11.8276 PSNR_train: 23.2857\n",
      "\n",
      "[epoch 14] PSNR_val: 24.3960\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 15] PSNR_val: 22.8049\n",
      "learning rate 0.001000\n",
      "[epoch 16][1/8] loss: 10.5981 PSNR_train: 22.5425\n",
      "\n",
      "[epoch 16] PSNR_val: 23.6606\n",
      "learning rate 0.001000\n",
      "[epoch 17][3/8] loss: 11.8095 PSNR_train: 19.5127\n",
      "\n",
      "[epoch 17] PSNR_val: 23.6514\n",
      "learning rate 0.001000\n",
      "[epoch 18][5/8] loss: 8.7918 PSNR_train: 24.4275\n",
      "\n",
      "[epoch 18] PSNR_val: 23.3541\n",
      "learning rate 0.001000\n",
      "[epoch 19][7/8] loss: 9.5080 PSNR_train: 21.5990\n",
      "\n",
      "[epoch 19] PSNR_val: 23.5155\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 20] PSNR_val: 22.5453\n",
      "learning rate 0.001000\n",
      "[epoch 21][1/8] loss: 9.2274 PSNR_train: 22.8377\n",
      "\n",
      "[epoch 21] PSNR_val: 22.4669\n",
      "learning rate 0.001000\n",
      "[epoch 22][3/8] loss: 7.5520 PSNR_train: 23.7627\n",
      "\n",
      "[epoch 22] PSNR_val: 23.6273\n",
      "learning rate 0.001000\n",
      "[epoch 23][5/8] loss: 8.0459 PSNR_train: 25.2325\n",
      "\n",
      "[epoch 23] PSNR_val: 20.9640\n",
      "learning rate 0.001000\n",
      "[epoch 24][7/8] loss: 7.7950 PSNR_train: 21.7857\n",
      "\n",
      "[epoch 24] PSNR_val: 22.9020\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 25] PSNR_val: 22.9840\n",
      "learning rate 0.001000\n",
      "[epoch 26][1/8] loss: 5.9222 PSNR_train: 25.7965\n",
      "\n",
      "[epoch 26] PSNR_val: 24.4002\n",
      "learning rate 0.001000\n",
      "[epoch 27][3/8] loss: 5.9075 PSNR_train: 26.8600\n",
      "\n",
      "[epoch 27] PSNR_val: 22.7762\n",
      "learning rate 0.001000\n",
      "[epoch 28][5/8] loss: 7.1166 PSNR_train: 25.6111\n",
      "\n",
      "[epoch 28] PSNR_val: 22.9239\n",
      "learning rate 0.001000\n",
      "[epoch 29][7/8] loss: 6.8153 PSNR_train: 24.0928\n",
      "\n",
      "[epoch 29] PSNR_val: 24.0418\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 30] PSNR_val: 23.5840\n",
      "learning rate 0.001000\n",
      "[epoch 31][1/8] loss: 5.3417 PSNR_train: 24.4164\n",
      "\n",
      "[epoch 31] PSNR_val: 21.8807\n",
      "learning rate 0.001000\n",
      "[epoch 32][3/8] loss: 5.4974 PSNR_train: 25.3701\n",
      "\n",
      "[epoch 32] PSNR_val: 24.3964\n",
      "learning rate 0.001000\n",
      "[epoch 33][5/8] loss: 4.7845 PSNR_train: 25.9576\n",
      "\n",
      "[epoch 33] PSNR_val: 23.8492\n",
      "learning rate 0.001000\n",
      "[epoch 34][7/8] loss: 5.0139 PSNR_train: 24.9668\n",
      "\n",
      "[epoch 34] PSNR_val: 21.4351\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 35] PSNR_val: 23.6042\n",
      "learning rate 0.001000\n",
      "[epoch 36][1/8] loss: 4.4868 PSNR_train: 25.2641\n",
      "\n",
      "[epoch 36] PSNR_val: 23.3438\n",
      "learning rate 0.001000\n",
      "[epoch 37][3/8] loss: 4.1741 PSNR_train: 25.9628\n",
      "\n",
      "[epoch 37] PSNR_val: 22.7772\n",
      "learning rate 0.001000\n",
      "[epoch 38][5/8] loss: 5.6700 PSNR_train: 27.1071\n",
      "\n",
      "[epoch 38] PSNR_val: 23.6487\n",
      "learning rate 0.001000\n",
      "[epoch 39][7/8] loss: 3.3998 PSNR_train: 27.2591\n",
      "\n",
      "[epoch 39] PSNR_val: 23.6451\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 40] PSNR_val: 21.5123\n",
      "learning rate 0.001000\n",
      "[epoch 41][1/8] loss: 3.5003 PSNR_train: 24.7051\n",
      "\n",
      "[epoch 41] PSNR_val: 22.6707\n",
      "learning rate 0.001000\n",
      "[epoch 42][3/8] loss: 4.0041 PSNR_train: 25.4061\n",
      "\n",
      "[epoch 42] PSNR_val: 23.1956\n",
      "learning rate 0.001000\n",
      "[epoch 43][5/8] loss: 3.4413 PSNR_train: 26.5199\n",
      "\n",
      "[epoch 43] PSNR_val: 23.7519\n",
      "learning rate 0.001000\n",
      "[epoch 44][7/8] loss: 3.4079 PSNR_train: 26.2005\n",
      "\n",
      "[epoch 44] PSNR_val: 24.5935\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 45] PSNR_val: 23.2735\n",
      "learning rate 0.001000\n",
      "[epoch 46][1/8] loss: 3.3238 PSNR_train: 26.2050\n",
      "\n",
      "[epoch 46] PSNR_val: 23.2801\n",
      "learning rate 0.001000\n",
      "[epoch 47][3/8] loss: 4.3310 PSNR_train: 25.2474\n",
      "\n",
      "[epoch 47] PSNR_val: 22.2247\n",
      "learning rate 0.001000\n",
      "[epoch 48][5/8] loss: 2.7914 PSNR_train: 28.7160\n",
      "\n",
      "[epoch 48] PSNR_val: 23.8009\n",
      "learning rate 0.001000\n",
      "[epoch 49][7/8] loss: 3.2262 PSNR_train: 21.7148\n",
      "\n",
      "[epoch 49] PSNR_val: 23.9710\n",
      "learning rate 0.001000\n",
      "\n",
      "[epoch 50] PSNR_val: 23.3584\n",
      "learning rate 0.001000\n",
      "[epoch 51][1/8] loss: 3.8441 PSNR_train: 25.5289\n",
      "\n",
      "[epoch 51] PSNR_val: 23.4008\n",
      "learning rate 0.000100\n",
      "[epoch 52][3/8] loss: 3.1445 PSNR_train: 31.1774\n",
      "\n",
      "[epoch 52] PSNR_val: 24.3785\n",
      "learning rate 0.000100\n",
      "[epoch 53][5/8] loss: 2.9152 PSNR_train: 29.6116\n",
      "\n",
      "[epoch 53] PSNR_val: 24.5893\n",
      "learning rate 0.000100\n",
      "[epoch 54][7/8] loss: 2.5045 PSNR_train: 31.0768\n",
      "\n",
      "[epoch 54] PSNR_val: 25.2329\n",
      "learning rate 0.000100\n",
      "\n",
      "[epoch 55] PSNR_val: 25.0684\n",
      "learning rate 0.000100\n",
      "[epoch 56][1/8] loss: 2.8312 PSNR_train: 31.6349\n",
      "\n",
      "[epoch 56] PSNR_val: 24.9226\n",
      "learning rate 0.000100\n",
      "[epoch 57][3/8] loss: 2.2170 PSNR_train: 32.8610\n",
      "\n",
      "[epoch 57] PSNR_val: 24.7519\n",
      "learning rate 0.000100\n",
      "[epoch 58][5/8] loss: 2.9081 PSNR_train: 32.7199\n",
      "\n",
      "[epoch 58] PSNR_val: 25.2042\n",
      "learning rate 0.000100\n",
      "[epoch 59][7/8] loss: 2.1515 PSNR_train: 33.3586\n",
      "\n",
      "[epoch 59] PSNR_val: 25.2751\n",
      "learning rate 0.000100\n",
      "\n",
      "[epoch 60] PSNR_val: 25.1985\n",
      "learning rate 0.000100\n",
      "[epoch 61][1/8] loss: 2.4460 PSNR_train: 32.9528\n",
      "\n",
      "[epoch 61] PSNR_val: 24.9659\n",
      "learning rate 0.000001\n",
      "[epoch 62][3/8] loss: 2.5451 PSNR_train: 32.9007\n",
      "\n",
      "[epoch 62] PSNR_val: 24.9630\n",
      "learning rate 0.000001\n",
      "[epoch 63][5/8] loss: 2.8386 PSNR_train: 32.6255\n",
      "\n",
      "[epoch 63] PSNR_val: 24.9446\n",
      "learning rate 0.000001\n",
      "[epoch 64][7/8] loss: 2.3420 PSNR_train: 33.5092\n",
      "\n",
      "[epoch 64] PSNR_val: 24.9675\n",
      "learning rate 0.000001\n",
      "\n",
      "[epoch 65] PSNR_val: 24.9850\n",
      "learning rate 0.000001\n",
      "[epoch 66][1/8] loss: 2.5671 PSNR_train: 33.0656\n",
      "\n",
      "[epoch 66] PSNR_val: 24.9849\n",
      "learning rate 0.000001\n",
      "[epoch 67][3/8] loss: 2.6584 PSNR_train: 32.8733\n",
      "\n",
      "[epoch 67] PSNR_val: 24.9610\n",
      "learning rate 0.000001\n",
      "[epoch 68][5/8] loss: 2.7893 PSNR_train: 32.8306\n",
      "\n",
      "[epoch 68] PSNR_val: 25.0219\n",
      "learning rate 0.000001\n",
      "[epoch 69][7/8] loss: 2.2350 PSNR_train: 33.6320\n",
      "\n",
      "[epoch 69] PSNR_val: 24.8626\n",
      "learning rate 0.000001\n",
      "\n",
      "[epoch 70] PSNR_val: 24.9068\n",
      "learning rate 0.000001\n",
      "[epoch 71][1/8] loss: 2.5029 PSNR_train: 32.9845\n",
      "\n",
      "[epoch 71] PSNR_val: 24.9728\n",
      "learning rate 0.000001\n",
      "[epoch 72][3/8] loss: 2.7412 PSNR_train: 32.8146\n",
      "\n",
      "[epoch 72] PSNR_val: 25.0370\n",
      "learning rate 0.000001\n",
      "[epoch 73][5/8] loss: 2.4562 PSNR_train: 34.4037\n",
      "\n",
      "[epoch 73] PSNR_val: 25.0375\n",
      "learning rate 0.000001\n",
      "[epoch 74][7/8] loss: 2.4213 PSNR_train: 32.9684\n",
      "\n",
      "[epoch 74] PSNR_val: 25.0341\n",
      "learning rate 0.000001\n",
      "\n",
      "[epoch 75] PSNR_val: 25.0464\n",
      "learning rate 0.000001\n",
      "[epoch 76][1/8] loss: 2.1839 PSNR_train: 34.1318\n",
      "\n",
      "[epoch 76] PSNR_val: 25.0249\n",
      "learning rate 0.000001\n",
      "[epoch 77][3/8] loss: 2.6889 PSNR_train: 33.2687\n",
      "\n",
      "[epoch 77] PSNR_val: 25.0511\n",
      "learning rate 0.000001\n",
      "[epoch 78][5/8] loss: 2.2396 PSNR_train: 33.6448\n",
      "\n",
      "[epoch 78] PSNR_val: 25.0949\n",
      "learning rate 0.000001\n",
      "[epoch 79][7/8] loss: 2.6541 PSNR_train: 34.0159\n",
      "\n",
      "[epoch 79] PSNR_val: 24.9650\n",
      "learning rate 0.000001\n",
      "\n",
      "[epoch 80] PSNR_val: 25.0233\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trains a FFDNet model\n",
    "\n",
    "By default, the training starts with a learning rate equal to 1e-3 (--lr).\n",
    "After the number of epochs surpasses the first milestone (--milestone), the\n",
    "lr gets divided by 100. Up until this point, the orthogonalization technique\n",
    "described in the FFDNet paper is performed (--no_orthog to set it off).\n",
    "\n",
    "Copyright (C) 2018, Matias Tassano <matias.tassano@parisdescartes.fr>\n",
    "\n",
    "This program is free software: you can use, modify and/or\n",
    "redistribute it under the terms of the GNU General Public\n",
    "License as published by the Free Software Foundation, either\n",
    "version 3 of the License, or (at your option) any later\n",
    "version. You should have received a copy of this license along\n",
    "this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as utils\n",
    "from tensorboardX import SummaryWriter\n",
    "from models import FFDNet\n",
    "from dataset import Dataset\n",
    "from utils import weights_init_kaiming, batch_psnr, init_logger, \\\n",
    "\t\t\tsvd_orthogonalization\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def main(args):\n",
    "\tr\"\"\"Performs the main training loop\n",
    "\t\"\"\"\n",
    "\t# Load dataset\n",
    "\tprint('> Loading dataset ...')\n",
    "\tdataset_train = Dataset(train=True, gray_mode=args.gray, shuffle=True)\n",
    "\tdataset_val = Dataset(train=False, gray_mode=args.gray, shuffle=False)\n",
    "\tloader_train = DataLoader(dataset=dataset_train, num_workers=6, \\\n",
    "\t\t\t\t\t\t\t   batch_size=args.batch_size, shuffle=True)\n",
    "\tprint(\"\\t# of training samples: %d\\n\" % int(len(dataset_train)))\n",
    "\n",
    "\t# Init loggers\n",
    "\tif not os.path.exists(args.log_dir):\n",
    "\t\tos.makedirs(args.log_dir)\n",
    "\twriter = SummaryWriter(args.log_dir)\n",
    "\tlogger = init_logger(args)\n",
    "\n",
    "\t# Create model\n",
    "\tif not args.gray:\n",
    "\t\tin_ch = 3\n",
    "\telse:\n",
    "\t\tin_ch = 1\n",
    "\tnet = FFDNet(num_input_channels=in_ch)\n",
    "\t# Initialize model with He init\n",
    "\tnet.apply(weights_init_kaiming)\n",
    "\t# Define loss\n",
    "\tcriterion = nn.MSELoss(size_average=False)\n",
    "\n",
    "\t# Move to GPU\n",
    "\tdevice_ids = [0]\n",
    "\tmodel = nn.DataParallel(net, device_ids=device_ids).cuda()\n",
    "\tcriterion.cuda()\n",
    "\n",
    "\t# Optimizer\n",
    "\toptimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "\t# Resume training or start anew\n",
    "\tif args.resume_training:\n",
    "\t\tresumef = os.path.join(args.log_dir, 'ckpt.pth')\n",
    "\t\tif os.path.isfile(resumef):\n",
    "\t\t\tcheckpoint = torch.load(resumef)\n",
    "\t\t\tprint(\"> Resuming previous training\")\n",
    "\t\t\tmodel.load_state_dict(checkpoint['state_dict'])\n",
    "\t\t\toptimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\t\t\tnew_epoch = args.epochs\n",
    "\t\t\tnew_milestone = args.milestone\n",
    "\t\t\tcurrent_lr = args.lr\n",
    "\t\t\targs = checkpoint['args']\n",
    "\t\t\ttraining_params = checkpoint['training_params']\n",
    "\t\t\tstart_epoch = training_params['start_epoch']\n",
    "\t\t\targs.epochs = new_epoch\n",
    "\t\t\targs.milestone = new_milestone\n",
    "\t\t\targs.lr = current_lr\n",
    "\t\t\tprint(\"=> loaded checkpoint '{}' (epoch {})\"\\\n",
    "\t\t\t\t  .format(resumef, start_epoch))\n",
    "\t\t\tprint(\"=> loaded parameters :\")\n",
    "\t\t\tprint(\"==> checkpoint['optimizer']['param_groups']\")\n",
    "\t\t\tprint(\"\\t{}\".format(checkpoint['optimizer']['param_groups']))\n",
    "\t\t\tprint(\"==> checkpoint['training_params']\")\n",
    "\t\t\tfor k in checkpoint['training_params']:\n",
    "\t\t\t\tprint(\"\\t{}, {}\".format(k, checkpoint['training_params'][k]))\n",
    "\t\t\targpri = vars(checkpoint['args'])\n",
    "\t\t\tprint(\"==> checkpoint['args']\")\n",
    "\t\t\tfor k in argpri:\n",
    "\t\t\t\tprint(\"\\t{}, {}\".format(k, argpri[k]))\n",
    "\n",
    "\t\t\targs.resume_training = False\n",
    "\t\telse:\n",
    "\t\t\traise Exception(\"Couldn't resume training with checkpoint {}\".\\\n",
    "\t\t\t\t   format(resumef))\n",
    "\telse:\n",
    "\t\tstart_epoch = 0\n",
    "\t\ttraining_params = {}\n",
    "\t\ttraining_params['step'] = 0\n",
    "\t\ttraining_params['current_lr'] = 0\n",
    "\t\ttraining_params['no_orthog'] = args.no_orthog\n",
    "\n",
    "\t# Training\n",
    "\tfor epoch in range(start_epoch, args.epochs):\n",
    "\t\t# Learning rate value scheduling according to args.milestone\n",
    "\t\tif epoch > args.milestone[1]:\n",
    "\t\t\tcurrent_lr = args.lr / 1000.\n",
    "\t\t\ttraining_params['no_orthog'] = True\n",
    "\t\telif epoch > args.milestone[0]:\n",
    "\t\t\tcurrent_lr = args.lr / 10.\n",
    "\t\telse:\n",
    "\t\t\tcurrent_lr = args.lr\n",
    "\n",
    "\t\t# set learning rate in optimizer\n",
    "\t\tfor param_group in optimizer.param_groups:\n",
    "\t\t\tparam_group[\"lr\"] = current_lr\n",
    "\t\tprint('learning rate %f' % current_lr)\n",
    "\n",
    "\t\t# train\n",
    "\t\tfor i, data in enumerate(loader_train, 0):\n",
    "\t\t\t# Pre-training step\n",
    "\t\t\tmodel.train()\n",
    "\t\t\tmodel.zero_grad()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# inputs: noise and noisy image\n",
    "\t\t\timg_train = data\n",
    "\t\t\tnoise = torch.zeros(img_train.size())\n",
    "\t\t\tstdn = np.random.uniform(args.noiseIntL[0], args.noiseIntL[1], \\\n",
    "\t\t\t\t\t\t\tsize=noise.size()[0])\n",
    "\t\t\tfor nx in range(noise.size()[0]):\n",
    "\t\t\t\tsizen = noise[0, :, :, :].size()\n",
    "\t\t\t\tnoise[nx, :, :, :] = torch.FloatTensor(sizen).\\\n",
    "\t\t\t\t\t\t\t\t\tnormal_(mean=0, std=stdn[nx])\n",
    "\t\t\timgn_train = img_train + noise\n",
    "\t\t\t# Create input Variables\n",
    "\t\t\timg_train = Variable(img_train.cuda())\n",
    "\t\t\timgn_train = Variable(imgn_train.cuda())\n",
    "\t\t\tnoise = Variable(noise.cuda())\n",
    "\t\t\tstdn_var = Variable(torch.cuda.FloatTensor(stdn))\n",
    "\n",
    "\t\t\t# Evaluate model and optimize it\n",
    "\t\t\tout_train = model(imgn_train, stdn_var)\n",
    "\t\t\tloss = criterion(out_train, noise) / (imgn_train.size()[0]*2)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Results\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tout_train = torch.clamp(imgn_train-model(imgn_train, stdn_var), 0., 1.)\n",
    "\t\t\tpsnr_train = batch_psnr(out_train, img_train, 1.)\n",
    "\t\t\t# PyTorch v0.4.0: loss.data[0] --> loss.item()\n",
    "\n",
    "\t\t\tif training_params['step'] % args.save_every == 0:\n",
    "\t\t\t\t# Apply regularization by orthogonalizing filters\n",
    "\t\t\t\tif not training_params['no_orthog']:\n",
    "\t\t\t\t\tmodel.apply(svd_orthogonalization)\n",
    "\n",
    "\t\t\t\t# Log the scalar values\n",
    "\t\t\t\twriter.add_scalar('loss', loss.data, training_params['step'])\n",
    "\t\t\t\twriter.add_scalar('PSNR on training data', psnr_train, \\\n",
    "\t\t\t\t\t  training_params['step'])\n",
    "\t\t\t\tprint(\"[epoch %d][%d/%d] loss: %.4f PSNR_train: %.4f\" %\\\n",
    "\t\t\t\t\t(epoch+1, i+1, len(loader_train), loss.data, psnr_train))\n",
    "\t\t\ttraining_params['step'] += 1\n",
    "\t\t# The end of each epoch\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\t# Validation\n",
    "\t\tpsnr_val = 0\n",
    "\t\tfor valimg in dataset_val:\n",
    "\t\t\timg_val = torch.unsqueeze(valimg, 0)\n",
    "\t\t\tnoise = torch.FloatTensor(img_val.size()).\\\n",
    "\t\t\t\t\tnormal_(mean=0, std=args.val_noiseL)\n",
    "\t\t\timgn_val = img_val + noise\n",
    "\t\t\timg_val, imgn_val = Variable(img_val.cuda()), Variable(imgn_val.cuda())\n",
    "\t\t\tsigma_noise = Variable(torch.cuda.FloatTensor([args.val_noiseL]))\n",
    "\t\t\tout_val = torch.clamp(imgn_val-model(imgn_val, sigma_noise), 0., 1.)\n",
    "\t\t\tpsnr_val += batch_psnr(out_val, img_val, 1.)\n",
    "\t\tpsnr_val /= len(dataset_val)\n",
    "\t\tprint(\"\\n[epoch %d] PSNR_val: %.4f\" % (epoch+1, psnr_val))\n",
    "\t\twriter.add_scalar('PSNR on validation data', psnr_val, epoch)\n",
    "\t\twriter.add_scalar('Learning rate', current_lr, epoch)\n",
    "\n",
    "\t\t# Log val images\n",
    "\t\ttry:\n",
    "\t\t\tif epoch == 0:\n",
    "\t\t\t\t# Log graph of the model\n",
    "\t\t\t\twriter.add_graph(model, (imgn_val, sigma_noise), )\n",
    "\t\t\t\t# Log validation images\n",
    "\t\t\t\tfor idx in range(2):\n",
    "\t\t\t\t\timclean = utils.make_grid(img_val.data[idx].clamp(0., 1.), \\\n",
    "\t\t\t\t\t\t\t\t\t\t\tnrow=2, normalize=False, scale_each=False)\n",
    "\t\t\t\t\timnsy = utils.make_grid(imgn_val.data[idx].clamp(0., 1.), \\\n",
    "\t\t\t\t\t\t\t\t\t\t\tnrow=2, normalize=False, scale_each=False)\n",
    "\t\t\t\t\twriter.add_image('Clean validation image {}'.format(idx), imclean, epoch)\n",
    "\t\t\t\t\twriter.add_image('Noisy validation image {}'.format(idx), imnsy, epoch)\n",
    "\t\t\tfor idx in range(2):\n",
    "\t\t\t\timrecons = utils.make_grid(out_val.data[idx].clamp(0., 1.), \\\n",
    "\t\t\t\t\t\t\t\t\t\tnrow=2, normalize=False, scale_each=False)\n",
    "\t\t\t\twriter.add_image('Reconstructed validation image {}'.format(idx), \\\n",
    "\t\t\t\t\t\t\t\timrecons, epoch)\n",
    "\t\t\t# Log training images\n",
    "\t\t\timclean = utils.make_grid(img_train.data, nrow=8, normalize=True, \\\n",
    "\t\t\t\t\t\t scale_each=True)\n",
    "\t\t\twriter.add_image('Training patches', imclean, epoch)\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tlogger.error(\"Couldn't log results: {}\".format(e))\n",
    "\n",
    "\t\t# save model and checkpoint\n",
    "\t\ttraining_params['start_epoch'] = epoch + 1\n",
    "\t\ttorch.save(model.state_dict(), os.path.join(args.log_dir, 'net.pth'))\n",
    "\t\tsave_dict = { \\\n",
    "\t\t\t'state_dict': model.state_dict(), \\\n",
    "\t\t\t'optimizer' : optimizer.state_dict(), \\\n",
    "\t\t\t'training_params': training_params, \\\n",
    "\t\t\t'args': args\\\n",
    "\t\t\t}\n",
    "\t\ttorch.save(save_dict, os.path.join(args.log_dir, 'ckpt.pth'))\n",
    "\t\tif epoch % args.save_every_epochs == 0:\n",
    "\t\t\ttorch.save(save_dict, os.path.join(args.log_dir, \\\n",
    "\t\t\t\t\t\t\t\t\t  'ckpt_e{}.pth'.format(epoch+1)))\n",
    "\t\tdel save_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\tparser = argparse.ArgumentParser(description=\"FFDNet\")\n",
    "\tparser.add_argument(\"--gray\", action='store_true',\\\n",
    "\t\t\t\t\t\thelp='train grayscale image denoising instead of RGB')\n",
    "\n",
    "\tparser.add_argument(\"--log_dir\", type=str, default=\"logs\", \\\n",
    "\t\t\t\t\t help='path of log files')\n",
    "\t#Training parameters\n",
    "\tparser.add_argument(\"--batch_size\", type=int, default=128, \t\\\n",
    "\t\t\t\t\t help=\"Training batch size\")\n",
    "\tparser.add_argument(\"--epochs\", \"--e\", type=int, default=80, \\\n",
    "\t\t\t\t\t help=\"Number of total training epochs\")\n",
    "\tparser.add_argument(\"--resume_training\", \"--r\", action='store_true',\\\n",
    "\t\t\t\t\t\thelp=\"resume training from a previous checkpoint\")\n",
    "\tparser.add_argument(\"--milestone\", nargs=2, type=int, default=[50, 60], \\\n",
    "\t\t\t\t\t\thelp=\"When to decay learning rate; should be lower than 'epochs'\")\n",
    "\tparser.add_argument(\"--lr\", type=float, default=1e-3, \\\n",
    "\t\t\t\t\t help=\"Initial learning rate\")\n",
    "\tparser.add_argument(\"--no_orthog\", action='store_true',\\\n",
    "\t\t\t\t\t\thelp=\"Don't perform orthogonalization as regularization\")\n",
    "\tparser.add_argument(\"--save_every\", type=int, default=10,\\\n",
    "\t\t\t\t\t\thelp=\"Number of training steps to log psnr and perform \\\n",
    "\t\t\t\t\t\torthogonalization\")\n",
    "\tparser.add_argument(\"--save_every_epochs\", type=int, default=5,\\\n",
    "\t\t\t\t\t\thelp=\"Number of training epochs to save state\")\n",
    "\tparser.add_argument(\"--noiseIntL\", nargs=2, type=int, default=[0, 75], \\\n",
    "\t\t\t\t\t help=\"Noise training interval\")\n",
    "\tparser.add_argument(\"--val_noiseL\", type=float, default=25, \\\n",
    "\t\t\t\t\t\thelp='noise level used on validation set')\n",
    "\targspar = parser.parse_args(\"\")\n",
    "\t# Normalize noise between [0, 1]\n",
    "\targspar.val_noiseL /= 255.\n",
    "\targspar.noiseIntL[0] /= 255.\n",
    "\targspar.noiseIntL[1] /= 255.\n",
    "\n",
    "\tprint(\"\\n### Training FFDNet model ###\")\n",
    "\tprint(\"> Parameters:\")\n",
    "\tfor p, v in zip(argspar.__dict__.keys(), argspar.__dict__.values()):\n",
    "\t\tprint('\\t{}: {}'.format(p, v))\n",
    "\tprint('\\n')\n",
    "\n",
    "\tmain(argspar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VII. utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Different utilities such as orthogonalization of weights, initialization of\n",
    "loggers, etc\n",
    "\n",
    "Copyright (C) 2018, Matias Tassano <matias.tassano@parisdescartes.fr>\n",
    "\n",
    "This program is free software: you can use, modify and/or\n",
    "redistribute it under the terms of the GNU General Public\n",
    "License as published by the Free Software Foundation, either\n",
    "version 3 of the License, or (at your option) any later\n",
    "version. You should have received a copy of this license along\n",
    "this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "def weights_init_kaiming(lyr):\n",
    "\tr\"\"\"Initializes weights of the model according to the \"He\" initialization\n",
    "\tmethod described in \"Delving deep into rectifiers: Surpassing human-level\n",
    "    performance on ImageNet classification\" - He, K. et al. (2015), using a\n",
    "    normal distribution.\n",
    "\tThis function is to be called by the torch.nn.Module.apply() method,\n",
    "\twhich applies weights_init_kaiming() to every layer of the model.\n",
    "\t\"\"\"\n",
    "\tclassname = lyr.__class__.__name__\n",
    "\tif classname.find('Conv') != -1:\n",
    "\t\tnn.init.kaiming_normal(lyr.weight.data, a=0, mode='fan_in')\n",
    "\telif classname.find('Linear') != -1:\n",
    "\t\tnn.init.kaiming_normal(lyr.weight.data, a=0, mode='fan_in')\n",
    "\telif classname.find('BatchNorm') != -1:\n",
    "\t\tlyr.weight.data.normal_(mean=0, std=math.sqrt(2./9./64.)).\\\n",
    "\t\t\tclamp_(-0.025, 0.025)\n",
    "\t\tnn.init.constant(lyr.bias.data, 0.0)\n",
    "\n",
    "def batch_psnr(img, imclean, data_range):\n",
    "\tr\"\"\"\n",
    "\tComputes the PSNR along the batch dimension (not pixel-wise)\n",
    "\n",
    "\tArgs:\n",
    "\t\timg: a `torch.Tensor` containing the restored image\n",
    "\t\timclean: a `torch.Tensor` containing the reference image\n",
    "\t\tdata_range: The data range of the input image (distance between\n",
    "\t\t\tminimum and maximum possible values). By default, this is estimated\n",
    "\t\t\tfrom the image data-type.\n",
    "\t\"\"\"\n",
    "\timg_cpu = img.data.cpu().numpy().astype(np.float32)\n",
    "\timgclean = imclean.data.cpu().numpy().astype(np.float32)\n",
    "\tpsnr = 0\n",
    "\tfor i in range(img_cpu.shape[0]):\n",
    "\t\tpsnr += peak_signal_noise_ratio(imgclean[i, :, :, :], img_cpu[i, :, :, :], \\\n",
    "\t\t\t\t\t   data_range=data_range)\n",
    "\treturn psnr/img_cpu.shape[0]\n",
    "\n",
    "def data_augmentation(image, mode):\n",
    "\tr\"\"\"Performs dat augmentation of the input image\n",
    "\n",
    "\tArgs:\n",
    "\t\timage: a cv2 (OpenCV) image\n",
    "\t\tmode: int. Choice of transformation to apply to the image\n",
    "\t\t\t0 - no transformation\n",
    "\t\t\t1 - flip up and down\n",
    "\t\t\t2 - rotate counterwise 90 degree\n",
    "\t\t\t3 - rotate 90 degree and flip up and down\n",
    "\t\t\t4 - rotate 180 degree\n",
    "\t\t\t5 - rotate 180 degree and flip\n",
    "\t\t\t6 - rotate 270 degree\n",
    "\t\t\t7 - rotate 270 degree and flip\n",
    "\t\"\"\"\n",
    "\tout = np.transpose(image, (1, 2, 0))\n",
    "\tif mode == 0:\n",
    "\t\t# original\n",
    "\t\tout = out\n",
    "\telif mode == 1:\n",
    "\t\t# flip up and down\n",
    "\t\tout = np.flipud(out)\n",
    "\telif mode == 2:\n",
    "\t\t# rotate counterwise 90 degree\n",
    "\t\tout = np.rot90(out)\n",
    "\telif mode == 3:\n",
    "\t\t# rotate 90 degree and flip up and down\n",
    "\t\tout = np.rot90(out)\n",
    "\t\tout = np.flipud(out)\n",
    "\telif mode == 4:\n",
    "\t\t# rotate 180 degree\n",
    "\t\tout = np.rot90(out, k=2)\n",
    "\telif mode == 5:\n",
    "\t\t# rotate 180 degree and flip\n",
    "\t\tout = np.rot90(out, k=2)\n",
    "\t\tout = np.flipud(out)\n",
    "\telif mode == 6:\n",
    "\t\t# rotate 270 degree\n",
    "\t\tout = np.rot90(out, k=3)\n",
    "\telif mode == 7:\n",
    "\t\t# rotate 270 degree and flip\n",
    "\t\tout = np.rot90(out, k=3)\n",
    "\t\tout = np.flipud(out)\n",
    "\telse:\n",
    "\t\traise Exception('Invalid choice of image transformation')\n",
    "\treturn np.transpose(out, (2, 0, 1))\n",
    "\n",
    "def variable_to_cv2_image(varim):\n",
    "\tr\"\"\"Converts a torch.autograd.Variable to an OpenCV image\n",
    "\n",
    "\tArgs:\n",
    "\t\tvarim: a torch.autograd.Variable\n",
    "\t\"\"\"\n",
    "\tnchannels = varim.size()[1]\n",
    "\tif nchannels == 1:\n",
    "\t\tres = (varim.data.cpu().numpy()[0, 0, :]*255.).clip(0, 255).astype(np.uint8)\n",
    "\telif nchannels == 3:\n",
    "\t\tres = varim.data.cpu().numpy()[0]\n",
    "\t\tres = cv2.cvtColor(res.transpose(1, 2, 0), cv2.COLOR_RGB2BGR)\n",
    "\t\tres = (res*255.).clip(0, 255).astype(np.uint8)\n",
    "\telse:\n",
    "\t\traise Exception('Number of color channels not supported')\n",
    "\treturn res\n",
    "\n",
    "def get_git_revision_short_hash():\n",
    "\tr\"\"\"Returns the current Git commit.\n",
    "\t\"\"\"\n",
    "\treturn subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip()\n",
    "\n",
    "def init_logger(argdict):\n",
    "\tr\"\"\"Initializes a logging.Logger to save all the running parameters to a\n",
    "\tlog file\n",
    "\n",
    "\tArgs:\n",
    "\t\targdict: dictionary of parameters to be logged\n",
    "\t\"\"\"\n",
    "\tfrom os.path import join\n",
    "\n",
    "\tlogger = logging.getLogger(__name__)\n",
    "\tlogger.setLevel(level=logging.INFO)\n",
    "\tfh = logging.FileHandler(join(argdict.log_dir, 'log.txt'), mode='a')\n",
    "\tformatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "\tfh.setFormatter(formatter)\n",
    "\tlogger.addHandler(fh)\n",
    "\ttry:\n",
    "\t\tlogger.info(\"Commit: {}\".format(get_git_revision_short_hash()))\n",
    "\texcept Exception as e:\n",
    "\t\tlogger.error(\"Couldn't get commit number: {}\".format(e))\n",
    "\tlogger.info(\"Arguments: \")\n",
    "\tfor k in argdict.__dict__:\n",
    "\t\tlogger.info(\"\\t{}: {}\".format(k, argdict.__dict__[k]))\n",
    "\n",
    "\treturn logger\n",
    "\n",
    "def init_logger_ipol():\n",
    "\tr\"\"\"Initializes a logging.Logger in order to log the results after\n",
    "\ttesting a model\n",
    "\n",
    "\tArgs:\n",
    "\t\tresult_dir: path to the folder with the denoising results\n",
    "\t\"\"\"\n",
    "\tlogger = logging.getLogger('testlog')\n",
    "\tlogger.setLevel(level=logging.INFO)\n",
    "\tfh = logging.FileHandler('out.txt', mode='w')\n",
    "\tformatter = logging.Formatter('%(message)s')\n",
    "\tfh.setFormatter(formatter)\n",
    "\tlogger.addHandler(fh)\n",
    "\n",
    "\treturn logger\n",
    "\n",
    "def init_logger_test(result_dir):\n",
    "\tr\"\"\"Initializes a logging.Logger in order to log the results after testing\n",
    "\ta model\n",
    "\n",
    "\tArgs:\n",
    "\t\tresult_dir: path to the folder with the denoising results\n",
    "\t\"\"\"\n",
    "\tfrom os.path import join\n",
    "\n",
    "\tlogger = logging.getLogger('testlog')\n",
    "\tlogger.setLevel(level=logging.INFO)\n",
    "\tfh = logging.FileHandler(join(result_dir, 'log.txt'), mode='a')\n",
    "\tformatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "\tfh.setFormatter(formatter)\n",
    "\tlogger.addHandler(fh)\n",
    "\n",
    "\treturn logger\n",
    "\n",
    "def normalize(data):\n",
    "\tr\"\"\"Normalizes a unit8 image to a float32 image in the range [0, 1]\n",
    "\n",
    "\tArgs:\n",
    "\t\tdata: a unint8 numpy array to normalize from [0, 255] to [0, 1]\n",
    "\t\"\"\"\n",
    "\treturn np.float32(data/255.)\n",
    "\n",
    "def svd_orthogonalization(lyr):\n",
    "\tr\"\"\"Applies regularization to the training by performing the\n",
    "\torthogonalization technique described in the paper \"FFDNet:\tToward a fast\n",
    "\tand flexible solution for CNN based image denoising.\" Zhang et al. (2017).\n",
    "\tFor each Conv layer in the model, the method replaces the matrix whose columns\n",
    "\tare the filters of the layer by new filters which are orthogonal to each other.\n",
    "\tThis is achieved by setting the singular values of a SVD decomposition to 1.\n",
    "\n",
    "\tThis function is to be called by the torch.nn.Module.apply() method,\n",
    "\twhich applies svd_orthogonalization() to every layer of the model.\n",
    "\t\"\"\"\n",
    "\tclassname = lyr.__class__.__name__\n",
    "\tif classname.find('Conv') != -1:\n",
    "\t\tweights = lyr.weight.data.clone()\n",
    "\t\tc_out, c_in, f1, f2 = weights.size()\n",
    "\t\tdtype = lyr.weight.data.type()\n",
    "\n",
    "\t\t# Reshape filters to columns\n",
    "\t\t# From (c_out, c_in, f1, f2)  to (f1*f2*c_in, c_out)\n",
    "\t\tweights = weights.permute(2, 3, 1, 0).contiguous().view(f1*f2*c_in, c_out)\n",
    "\n",
    "\t\t# Convert filter matrix to numpy array\n",
    "\t\tweights = weights.cpu().numpy()\n",
    "\n",
    "\t\t# SVD decomposition and orthogonalization\n",
    "\t\tmat_u, _, mat_vh = np.linalg.svd(weights, full_matrices=False)\n",
    "\t\tweights = np.dot(mat_u, mat_vh)\n",
    "\n",
    "\t\t# As full_matrices=False we don't need to set s[:] = 1 and do mat_u*s\n",
    "\t\tlyr.weight.data = torch.Tensor(weights).view(f1, f2, c_in, c_out).\\\n",
    "\t\t\tpermute(3, 2, 0, 1).type(dtype)\n",
    "\telse:\n",
    "\t\tpass\n",
    "\n",
    "def remove_dataparallel_wrapper(state_dict):\n",
    "\tr\"\"\"Converts a DataParallel model to a normal one by removing the \"module.\"\n",
    "\twrapper in the module dictionary\n",
    "\n",
    "\tArgs:\n",
    "\t\tstate_dict: a torch.nn.DataParallel state dictionary\n",
    "\t\"\"\"\n",
    "\tfrom collections import OrderedDict\n",
    "\n",
    "\tnew_state_dict = OrderedDict()\n",
    "\tfor k, vl in state_dict.items():\n",
    "\t\tname = k[7:] # remove 'module.' of DataParallel\n",
    "\t\tnew_state_dict[name] = vl\n",
    "\n",
    "\treturn new_state_dict\n",
    "\n",
    "def is_rgb(im_path):\n",
    "\tr\"\"\" Returns True if the image in im_path is an RGB image\n",
    "\t\"\"\"\n",
    "\tfrom skimage.io import imread\n",
    "\trgb = False\n",
    "\tim = imread(im_path)\n",
    "\tif (len(im.shape) == 3):\n",
    "\t\tif not(np.allclose(im[...,0], im[...,1]) and np.allclose(im[...,2], im[...,1])):\n",
    "\t\t\trgb = True\n",
    "\tprint(\"rgb: {}\".format(rgb))\n",
    "\tprint(\"im shape: {}\".format(im.shape))\n",
    "\treturn rgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "08/03/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easydict\n",
      "  Downloading easydict-1.13-py3-none-any.whl.metadata (4.2 kB)\n",
      "Downloading easydict-1.13-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
